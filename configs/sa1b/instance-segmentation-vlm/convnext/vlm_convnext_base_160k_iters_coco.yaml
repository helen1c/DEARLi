_BASE_: ../maskformer2_R50_bs16_160k.yaml
MODEL:
  META_ARCHITECTURE: "VisionLanguageMaskFormer"
  SEM_SEG_HEAD:
    NAME: "VisionLanguageMaskFormerHead"
  # backbone part.
  BACKBONE:
    NAME: "OpenCLIPBackbone"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  VL_MASK_FORMER:
    OPEN_CLIP_MODEL_NAME: "convnext_base_w_320"
    OPEN_CLIP_PRETRAINED_WEIGHTS: "laion_aesthetic_s13b_b82k_augreg"
    EMBED_DIM: 640
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    FREEZE_BACKBONE: True
    INCLUDE_MASK_POOLINGS: False
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 200
    CLASS_WEIGHT: 0.0
    TEST:
      SEMANTIC_ON: False
      INSTANCE_ON: True
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0


SOLVER:
  MAX_ITER: 160000
  CHECKPOINT_PERIOD: 160000
  IMS_PER_BATCH: 16

# we do not evaluate the model during training
TEST:
  EVAL_PERIOD: 160001

DATASETS:
  TRAIN: ("vlm_coco_whole_sa1b_train",)